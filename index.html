<!DOCTYPE html>
<html >
<head>
  <meta charset="UTF-8">
  <title>Activation Functions</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/5.0.0/normalize.min.css">

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script src='http://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js'></script>
<!--<script src='https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/3.0.4/jquery.imagesloaded.min.js'></script> -->
<script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>


    <script  src="js/index.js"></script>

  
      <style>
      /* NOTE: The styles were added inline because Prefixfree needs access to your styles and they must be inlined if they are on local disk! */
      html {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-size: 16px;
}

body {
  color: #333;
  height: 100%;
  line-height: 1.5em;
  margin-bottom: 1.5em;
}

main {
  margin: 0 auto;
  width: 66.66667%;
}

header {
  background-color: #333;
  background-image: linear-gradient(to bottom, transparent 0%, rgba(0, 0, 0, 0.25) 100%);
  box-sizing: border-box;
  color: #eee;
  margin-bottom: 1.5rem;
  overflow: hidden;
  padding: 3rem 25%;
  pointer-events: none;
  position: relative;
  left: -25%;
  width: 150%;
  transition: all .25s ease-in-out;
}
header:before {
  background-image: linear-gradient(115deg, rgba(255, 255, 255, 0.025) 0%, rgba(255, 255, 255, 0.05) 50%, rgba(255, 255, 255, 0) 50%), linear-gradient(to left, #005952 0%, #005E20 100%);
  content: "";
  opacity: 0;
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  transition: all .25s ease-in-out;
}
header:hover:before {
  opacity: 1;
}
header > * {
  position: relative;
}
header h1 {
  font-weight: bold;
  font-size: 3em;
  line-height: 1em;
  margin: 0 0 .5rem 0;
  text-align: center;
}
header .byline {
  color: rgba(255, 255, 255, 0.75);
  display: block;
  font-size: .8em;
  letter-spacing: 1px;
  text-align: center;
  text-transform: uppercase;
}

p {
  margin-bottom: 1.5rem;
}

.message {
  background-color: #FDC68A;
  border-left: .75rem solid #F26C4F;
  padding: .75rem 1.5rem;
  font-size: .85rem;
}
.message a:hover {
  border-color: rgba(0, 0, 0, 0.5);
  color: rgba(0, 0, 0, 0.5);
}
.message.note {
  background-color: #7BCDC8;
  border-color: #00A99D;
}

hr {
  border: none;
  border-top: .125rem solid #ddd;
  margin-bottom: 1.375rem;
}

a {
  border-bottom: 2px solid #333;
  color: #333;
  text-decoration: none;
  transition: all .125s ease-in-out;
}
a:hover {
  border-bottom-color: #4a7298;
  color: #4a7298;
}

ul {
  margin-top: -.75rem;
}

code {
  background-color: rgba(0, 0, 0, 0.125);
  border: 1px solid rgba(0, 0, 0, 0.125);
  border-radius: 4px;
  display: inline-block;
  margin: 0 .125em;
  padding: 0 .25em;
}

/* Cards */
.cards {
  border-radius: .25rem;
  display: flex;
  flex-flow: row nowrap;
  margin-bottom: 1.5rem;
  overflow: hidden;
}
.cards > a {
  box-sizing: border-box;
  border: none;
  flex: 1 100%;
  padding: 1.5rem;
  text-align: center;
}
.cards > a svg {
  display: block;
  margin: 0 auto .75rem;
  width: 5.25rem;
  height: 5.25rem;
  transition: all .125s ease-in-out;
}
.cards > a svg path {
  transition: fill 0.125s ease-in-out;
}
.cards > a {
  background-color: #333;
  color: #eee;
}
.cards > a svg {
  background-color: #eee;
}
.cards > a svg path {
  fill: #333;
}
.cards > a:hover {
  background-color: #222;
  color: #57ad68;
}
.cards > a:hover svg {
  background-color: #57ad68;
}
.cards > a:hover svg path {
  fill: #222;
}

/* Fluidbox styling starts here */
a[data-fluidbox] {
  background-color: #eee;
  border: none;
  cursor: -webkit-zoom-in;
  cursor: -moz-zoom-in;
  margin-bottom: 1.5rem;
}
a[data-fluidbox].fluidbox-opened {
  cursor: -webkit-zoom-out;
  cursor: -moz-zoom-out;
}
a[data-fluidbox] img {
  display: block;
  margin: 0 auto;
  opacity: 0;
  max-width: 100%;
  transition: all .25s ease-in-out;
}

a[class^='float'] {
  margin: 1rem;
  margin-top: 0;
  width: 33.33333%;
}
a[class^='float'].float-left {
  float: left;
  margin-left: 0;
}
a[class^='float'].float-right {
  float: right;
  margin-right: 0;
}

#fluidbox-overlay {
  background-color: rgba(255, 255, 255, 0.85);
  cursor: pointer;
  cursor: -webkit-zoom-out;
  cursor: -moz-zoom-out;
  display: none;
  position: fixed;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
  z-index: 500;
}

.fluidbox-wrap {
  background-position: center center;
  background-size: cover;
  margin: 0 auto;
  position: relative;
  z-index: 400;
  transition: all .25s ease-in-out;
}
.fluidbox-opened .fluidbox-wrap {
  z-index: 600;
}

.fluidbox-ghost {
  background-size: cover;
  background-position: center center;
  position: absolute;
  transition: all .25s ease-in-out;
}

.gallery {
  display: flex;
  -webkit-flex-flow: row wrap;
  /* Fix for iOS */
  flex-flow: row wrap;
  justify-content: space-between;
}
.gallery a {
  margin-bottom: 1rem;
}
.gallery a.col-2 {
  width: 49%;
}
.gallery a.col-3 {
  width: 32%;
}

/* Media queries, unrelated to functionality of Fluidbox */
@media only screen and (max-width: 768px) {
  .cards {
    flex-flow: row wrap;
  }

  a[class^='float'] {
    width: 50%;
  }
}
@media only screen and (max-width: 600px) {
  .gallery a,
  .gallery a[class^='col'] {
    margin-bottom: .5rem;
    width: 100%;
  }
}
@media only screen and (max-width: 480px) {
  html {
    font-size: 12px;
  }

  .message {
    border: 0;
    border-top: .75rem solid #F26C4F;
  }

  a[class^='float'] {
    float: none;
    margin: 0 0 1rem 0;
    width: 100%;
  }
}
.latexCode {
  background-color: rgba(0, 0, 0, 0.125);
  border: 1px solid rgba(0, 0, 0, 0.125);
  border-radius: 4px;
  display: inline-block;
  margin: 0 .125em;
  padding: 0 .25em;
}

    </style>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/prefixfree/1.0.7/prefixfree.min.js"></script>

</head>

<body>
  <main>
  <header>
    <h1>Activation Functions</h1>
    <span class="byline">A Deeper Understanding</span>
  </header>
  <h1>What is an Activation Function?</h1>
  <p>An activation function is a mathematical function that is applied to the linear combination of weights and input values within a node in a hidden layer.</p>
  <center><img src="Images/nn.png" style="width:320px;height:225px"/></center>
  <p>Above we see that we have the input values $X1, X2, X3$ and the weights $W1, W2, W3$. There is one hidden layer with one node. In this node we take the linear combination of the inputs and weights to calculate</p>

  <center><p class="latexCode">
  $$\sum_{i=1}^3W_iX_i = W1X1 + W2X2 + W3X3$$</p></center>
  <br>
  <p>
  Additionally, there is a bias term added to this linear combination. This allows the neural network to translate its decision boundary. If we let our bias be denoted as $b_1$ in the above example then our neuron in the hidden layer computes </p>


  <center><p class="latexCode">$$W1X1 + W2X2 + W3X3 + b_1$$</p></center>
  <br>

  <p>We see that the above quantity is linear. However, there is a good chance that our dataset is non-linear and we want to learn a non-linear mapping between our inputs and targets. To introduce this non-linearity into the network, we apply a mathemaitcal operation $f$, known as an activation function, on the computed linear combination. Applying this activation function leaves us with the following quantity</p> 

  <center><p class="latexCode">$$f\left(W1X1 + W2X2 + W3X3 + b_1\right)$$</p></center>
  <br>
  <p>There are many different activation functions used, each with their own formulas, properties, pros, and cons. Understanding these functions at a mathematical level allows one to properly identify which activation functions to use in their deep learning models.</p>

  <h2>The Standard Activation Functions</h2>
  <p>
  Below are the names and formulas for some very popular activation functions used in deep learning:
  </p>
  <div class="cards">
    <a>
      <span>Linear<br></span>
      <center><p style="text-align:center">$f(x)=x$</p></center>
    </a>
    <a>
      <span>Sigmoid<br></span>
      <center><p style="text-align:center">$f(x)=\frac{1}{1+e^{-x}}$</p></center>
    </a>
    <a>
      <span>Tanh<br></span>
      <p style="text-align:center">$f(x)=\frac{2}{1+e^{-2x}-1}$</p>
    </a>
    </div>
    <div class="cards">
    <a>
      <span>ReLU<br></span>
      <center><p style="text-align:center">$f(x) = max(0,x)$</p></center>
    </a>
    <a>
      <span>Leaky ReLU<br></span>
      <p style="text-align:center">$f(x)=max(x,k\cdot z)$<br> <center>$0 < k < 1$</center></p>
    </a>
    <a>
      <span>Softmax<br></span>
      <p style="text-align:center">$f(x)_a = \frac{e^{x_a}}{\sum_{a=1}^ne^{x_a}}$</p>
    </a>
  </div>
<p>Below are plots of the activation functions with their respective derivatives:</p> 
<div class="gallery" style="background-color:white">
	<a style="background-color:white" href="" title="" data-fluidbox class="col-3"><center><h2>Linear</h2></center><img src="Images/linear.png" alt="" title="" /></a>
    <a style="background-color:white" href="" title="" data-fluidbox class="col-3"><center><h2>Sigmoid</h2></center><img src="Images/sigmoid.png" alt="" title="" /></a>
    <a style="background-color:white" href="" title="" data-fluidbox class="col-3"><center><h2>tanh</h2></center><img src="Images/tanh.png" alt="" title="" /></a>
    <a style="background-color:white" href="" title="" data-fluidbox class="col-3"><center><h2>ReLu</h2></center><img src="Images/relu.png" alt="" title="" /></a>
    <a style="background-color:white" href="" title="" data-fluidbox class="col-3"><center><h2>Leaky ReLu</h2></center><img src="Images/leaky_relu.png" alt="" title="" /></a>
    <a style="background-color:white" href="" title="" data-fluidbox class="col-3"><center><h2>Softmax</h2></center><center><img src="Images/softmax.png" alt="" title="" /></center></a>
  </div>
  <p>Let us now dive into each of the above activation functions and examine them at a deeper level:</p>

  <h2>The Linear:</h2>
  <h4>Properties:</h4>
  <p>The Linear activation function is a function which produces a node output equal to its input. It is a <b>linear</b> and <b>monotonic</b> function that has range $-\infty$ to $\infty$.
  <h4>Drawbacks:</h4>
  <ul>
  <li>Non-Linearity - The linear function is insufficient when non-linearities are present in the data.</li>
  </ul>
  <h4>Benefits:</h4>
  <ul>
  <li>No Vanishing Gradient - There is no vanishing gradient with a linear activation function.</li>
  <li>No Exploding Gradient - There is no exploding of the gradient with a linear activation function.</li>
  <li>No Saturated Neurons - There is no saturated neurons with a linear activation function.</li>
  <li>No Dead Neurons - There are no dead nuerons with a linear activation function.</li>
  </ul>
  <h4>Conclusion:</h4>
  <p>The linear activation function is perfectly suited to tasks where we have a linear behavior in our data. We typically use a linear activation in the output layer of our neural network if we have a regression problem. The linear activation is not commonly used in the hidden layers.

  <h2>The Sigmoid:</h2>
  <h4>Properties:</h4>
  <p>The sigmoid is a <b>smooth</b> and <b>non-linear</b> <b>continuously differentiable</b> function that squashes real numbers to range between 0 and 1. It has an S shaped curve that is <b>monotonic</b>. Additionally, the sigmoid is <b>asymmetrical</b> around the origin.</p>

  <h4>Drawbacks:</h4>
  <ul>
  <li>Vanishing Gradient - When we apply a large negative to the sigmoid we get close to a 0 value. When we apply a large positive to the sigmoid we get close to a 1 value. If we look at the gradients at these tail regions we see that we effectively have a 0 value. During backpropogation we multiply the local gradient to the gradient of the previous gate's output. If we have a small local gradient we will have a vanishing gradient and will thus have no signal for weight update. Thus, the sigmoid function suffers from the vanishing gradient problem as the learning process stops and the weights stop updating with increasing epochs as the gradient has fallen to 0. There needs to be a nonzero gradient for the weights to be updated.</li>
  <li>Saturated Neurons - When we initialize the weights to our network we have to be careful. If our weight initialization is too large then most neurons become saturated thus not allowing the network to properly learn. This can cause class imbalance irrespective of the input sample.</li>
  <li>Non-Zero Centered Outputs - Since our range is between 0 and 1 we always have positive data coming into the next neuron. This will cause the gradient on the weights to become all positive or all negative duing backpropogation.</li>
  </ul>
  <p>When we apply a large negative number to the sigmoid we get a 0 value and when we apply large positive numbers to the sigmoid we get a 1. When we look at these extreme tail regions we see that the gradient is almost 0. As we apply sigmoid after sigmoid we see that</p> 
  <h4>Benefits:</h4>
  <ul>
  <li>No Exploding Gradients - The sigmoid function does not cause an explosion in gradients when activation after activation function is applied. We see that the maximum range the function can take on is 1 after the sigmoid is applied and that the derivative peaks out at the range 0.25 for any real number so the gradients will never explode in magnitude.</li>
  </ul>
  <h4>Conclusion:</h4>
  <p>The sigmoid function has been widely used for a long time. In general it was used for cases where we are predicting the probability as an output since it outputs values in the range of 0,1 and probabilities only exist between this range. The probabilities from the sigmoid do not always sum to one so using this activation function can be useful in cases where there are multiple true answers instead of one correct classification or multiclass classification. For instance, we would want to use the sigmoid function in the output layer if we showed a network a picture that contained multiple cars and humans. Here we would want the network to produce a probability that each car was a car and that each human was a car. Here we do not need all the probabilites to sum to 1. This would allow us to detect the multiple cars in the image and discern them from the people in the image. We would not want to use the sigmoid if we have a multiclass classification problem where there was just one correct answer for which class is present in the image or data. For instance, we would not use the sigmoid to determine if an image is a cat or a dog or a bird, since we would desire the output probability to sum to one, thus not making it a probability distribution. There is an alternate activation function called the softmax which performs this, thus the sigmoid is not desirable in the case of a multiclass classification problem. However, if we have a binary classification problem the sigmoid and softmax will perform equally well with the sigmoid function being a more simple operation. Additionally, we would not use the sigmoid activation function in the output layer for a regression problem since we would desire an unbounded range and the sigmoid is constrained between the range of 0 to 1. Thus, if the sigmoid is used it is used in the output layer if we are seeking the probability of multiple "true" answers such as the presence of various objects in an image. Additionally, we could use it for the output layer in a binary classification problem. However, it should most likely not be used in hidden layers.</p> 




  <h2>The Tanh:</h2>
  <h4>Properties:</h4>
  <p>The hyperbolic tangent, or tanh, is a <b>smooth</b> and <b>non-linear</b> <b>continuously differentiable</b> function that squashes real numbers to range between -1 and 1. It has an S shaped curve that is <b>monotonic</b>.The tanh function is mathematically a <b>shifted</b> version of the sigmoid function such that it goes through the origin, making it <b>zero-centered</b>.</p>

  <h4>Drawbacks:</h4>
  <ul>
  <li>Vanishing Gradients - Like the sigmoid, the tanh has the problem of vanishing gradients.</li>
  <li>Saturated Neurons - Like the sigmoid, the tanh has the probelem of saturated gradients.</li>
  </ul>

  <h4>Benefits:</h4>
  <ul>
  <li>Hidden Units - For hidden units, the tanh almost always works better than the sigmoid function because we produce values between -1 and 1 compared to values between 0 and 1. Here, the mean of the activations that come out of the hidden layers are closer to having a zero mean. This allows us to have stronger gradients since the data is centered around 0 which makes the derivatives higher. If we look at the derivative of the sigmoid function we see we take on a range between 0 and 0.25. With the tanh's derivative we take on values between a range of 0 and 1. The tanh non-linearity is usually always preferred to the sigmoid nonlinearity.</li>
  <li>No Exploding Gradients - Like the sigmoid, the tanh does not have the problem of exploding gradients.</li>
  </ul>

  <h4>Conculsion:</h4>
  <p>The tanh is preferred to the sigmoid when we are choosing what activations we should use in the hidden layers and this is highly due to the zero-centered activation value property. It is still not as desirable as some alternate activation function called the ReLu but is preferred over the sigmoud. The tanh should not be used in the output layer if an output probability is desired as it produces values in the range -1 to 1 not 0 to 1 like the sigmoid. It can be used to discern classes in a binary classification problem as negative values can learn more towards one class and positive values can learn more towards another class. However, for classification tasks the softmax activation is preferred with the sigmoid trailing behind it. In summary, the tanh is preferred to the sigmoid for use in the hidden layers but for classification and regression problems it is not as desirable for use in the output layer.</p>

  <h2>The ReLU:</h2>

  <h4>Properties:</h4>
  <p>The rectified linear unit, or ReLu, is a <b>piece-wise linear</b> function. It has range 0 to $\infty$ and is <b>monotonic</b>. Additionally it is <b>Asymmetrical</b>. It has non-zero derivative at positive inputs and zero derivative at non-positive inputs. The ReLU is simply a threshold at zero.</p>
  <h4>Drawbacks:</h4>
  <ul>
  <li>Dead Neurons - During training, ReLU units can die. If we have a large gradient that is flowing through a ReLU neuron this can cause the weights to update in a way such that the neuron will never activate on any datapoint again. This will cause the gradient to forever be zero from that point on and the weights to not be updated during gradient descent. The sigmoid and tanh functions at least always have a small gradient allowing them to recover in the long term. Once the neuron dies with a ReLU it is very hard to recover. </li>
  </ul>
  <h4>Benefits:</h4>
  <ul>
  <li>No Vanishing Gradient - The ReLU has no vanishing gradient problem like the sigmoid or tanh. </li>
  <li>No Exploding Gradient - The ReLU does not explode the gradient.</li>
  <li>No Saturated Neurons - The ReLU does not produce saturated neurons. It also has a linear form that speeds up the convergence of stochastic gradient descent compared to the sigmoid and tanh functions.</li>
  <li>No Complex Mathematical Operations - Compared to the sigmoid or tanh activation functions, the ReLU does not involve complex operations like expoentials. It is a simple operation by simply thresholding a matrix of activations at zero.</li>
  </ul>
  <h4>Conclusion:</h4>
  <p>The ReLU is the most common activation function chosen to be used inside hidden layers. It is generally better to use than the sigmoid or tanh functions in the hidden layers. If one is confused with what activation function to use in the hidden layers, the ReLU is a great first choice. ReLU activation functions should only be used in the hidden layers as we prefer output layers to have softmax activations for classification problems (or sigmoid for binary classification) and regression problems to have a linear activation on the output layer.</p>

  <h2>The Leaky ReLU:</h2>
  <h4>Properties:</h4>
  <p>The Leaky ReLU is a ReLU function that drops slightly below the x axis for negative values (usually scaling $x$ by $0.01$ for $x < 0$), this having a very small slope for these negative value inputs. Just like the ReLU it is a <b>piece-wise linear</b> function that is <b>monotonic</b> and <b>asymmetrical</b>. It has range $-\infty$ to $\infty$.
  <h4>Drawbacks:</h4>
  <li>Vanishing Gradient - The Leaky ReLU does have the problem of a vanishing gradient.</li>
  <h4>Benefits:</h4>
  <li>No Dead Neurons - The Leaky ReLU has a small negative slope for negative valued inputs. Thus, we have at least some small gradient and fix the dying neuron problem.</li>
  <li>No Exploding Gradient - Like the ReLU, there is no exploding gradient.</li>
  <li>No Saturated Neurons - Like the ReLU, there is no exploding gradient.</li>
  <h4>Conclusion:</h4>
  <p>The Leaky ReLU is a good choice for hidden layer units if one experiences a vanishing gradient. In general the ReLU is used more often in the hidden layers compared to the Leaky ReLU. The Leaky ReLU is not used in the output layers for classification and regression probelms.</p>

  <h2>The Softmax:</h2>
  <p>The softmax is a logistic function with the condition that the probabilities values from the application of the softmax must sum to 1. This makes it highly desirable for the output layer in a multiclass or binary classification problem. The softmax is generally not used in hidden layers as it can introudce linear dependence on our nodes which can result in problems. However, a softmax can be used in the hidden layer if we are building an attention mechanism.</p>
  


  <h2>What to use:</h2>
  <p>In summary we should use a softmax activation for a multiclass or binary classification problem in the output layer. We could use a sigmoid in the output layer for a binary classification problem. If we have a regression problem we should use a linear activation in the output layer. In the hidden layers we should try a ReLU activation but shift to a Leaky ReLU if we experience dead neurons. The ReLU and leaky ReLU should not be used in the output layers for our classification or regression problems. The tanh can be used in the hidden layers but a ReLU is preferred over it.</p>

<br><br><br><br>


</main>


</body>
</html>
